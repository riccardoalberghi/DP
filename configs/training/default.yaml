# Training parameters
num_epochs: 30  # Number of training epochs to run
tokens_per_epoch: 134217728 # Total number of tokens to process per epoch (128M tokens), it is 33554432 for 32M tokens
tokens_per_step: 16384 # =1024 * 16  # Number of tokens to process in each training step (16K tokens)
max_length: 4096  # Maximum sequence length for input samples (ie context length)

# Limit on number of training samples to use (null = no limit)
limit_num_of_samples: null

# Optimizer settings
learning_rate: 2e-5 # Learning rate for optimizer
constant_lr: True  # Whether to use constant learning rate (no schedule/decay)
weight_decay: 0.1  # Weight decay (L2 regularization) coefficient
logging_steps: 1  # Log metrics every N training steps

# Training behavior flags
train_on_cots: True  # Whether to train on chain-of-thought reasoning data

# Data augmentation/noise parameters
redundacy: 0  # Number of deterministic redundancy to add
p_redundancy: 0.0  # Probability of reinsering the path in the priority queue (ie random redundancy)

# Sampling temperature
temperature: 0.0  # Temperature for sampling (0.0 = greedy/deterministic)

# Path to checkpoint directory to resume training from, null means start from scratch
resume_from_checkpoint: null  # Checkpoint path to resume from (null = train from scratch)