# Training parameters
num_epochs: 30
tokens_per_epoch: 33554432 # 1024*1024*16 # 201326592 # 1024*1024*192
tokens_per_step: 16384 # 1024 * 16
# mini_batch_size: 2
max_length: 2048

limit_num_of_samples: null # 300000

learning_rate: 2e-5 # 5e-4 # 3e-5 # 
constant_lr: True
weight_decay: 0.1
logging_steps: 1

train_on_cots: True
use_wait: True
use_aha: True