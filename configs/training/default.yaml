# Training parameters
num_epochs: 30  # Number of training epochs to run
tokens_per_epoch: 134217728 # Total number of tokens to process per epoch (128M tokens), set to 33554432 for 32M tokens
tokens_per_step: 16384 # =1024 * 16  # Number of tokens to process in each training step
max_length: 4096  # Maximum sequence length for input samples (ie context length)

# Limit the maximum number of training samples to use (null = no limit, use all the tokens generated)
limit_num_of_samples: null

# Optimizer settings
learning_rate: 2e-5 # Learning rate for optimizer
constant_lr: True  # Whether to use constant learning rate (if no cosine scheduler)
weight_decay: 0.1  # Weight decay coefficient
logging_steps: 1  # Log metrics every N training steps

# Training behavior flags
train_on_cots: True  # Whether to train on chain-of-thought reasoning data

# CoT redundancy
redundacy: 0  # Number of times to multiply each CoT step deterministically. Used for Deterministic Redundancy. 0 and 1 have the same effect.
p_redundancy: 0.0  # Probability of reinsering the path in the priority queue (ie Random Redundancy)

# Sampling temperature on test set
temperature: 0.0  # Temperature for sampling (0.0 = greedy/deterministic)

# Path to checkpoint directory to resume training from, null means start from scratch
resume_from_checkpoint: null